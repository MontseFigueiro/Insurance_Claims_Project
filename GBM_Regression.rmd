---
title: "Regresión con GBM"
author: "Montse Figueiro & Aniana González"
date: "27 de octubre de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##REGRESIÓN CON GBM (Gradient Boosted Machine)
```{r}
library(caret)
library(gbm)
memory.limit(90000)
```

###Carga de Datos

Utilizamos las mismas 5000 observaciones que habíamos utilizado en SVM para poder hacer la comparativa de los modelos.

```{r}
sample_train <- read.csv("sample_train_5000.csv")
sample_train_down <- read.csv("sample_train_down.csv")
test <- read.csv("testdf.csv")
```

###Fichero Test para Validación

```{r}
cols <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1", "Var2", "Var3", "Var4", "Var5", "Var6","Var7", "Var8", "NVCat", "NVVar1", "NVVar2", "NVVar3", "NVVar4")
test_gbm <- test[,cols]
test_gbm_output <- test[,"Claim_Amount"]
```


##GBM REGRESIÓN CON DATOS DESEQUILIBRADOS

Boosting is an ensemble method developed for classification for reducing bias where models are added to learn the misclassification errors in existing models. It has been generalized and adapted in the form of Gradient Boosted Machines (GBM) for use with CART decision trees for classification and regression.


###SIN LOG TRANSFORMACIÓN

```{r}
model_gbm1 <- gbm(Claim_Amount~., data=sample_train,distribution="gaussian",n.trees = 1000)
prediction1 <- predict(model_gbm1,test_gbm,n.trees=1000)
sqrt(mean((test_gbm_output-prediction1)^2))
```
RSME = 48.13307


###CON LOG TRANSFORMACIÓN

```{r}
model_gbm2 <- gbm(log(Claim_Amount+1)~., data=sample_train,distribution="gaussian",n.trees = 1000)
prediction2 <- predict(model_gbm2,test_gbm,n.trees=1000)
sqrt(mean((test_gbm_output-prediction2)^2))
```
RSME = 39.35099

##K-FOLD CROSS-VALIDATION CON CARET 

Si lo ejecutamos con method= "svmLinear" los resultados son los mismos que ejecutando CVM con el paquete "e1071". 

###Sin LOG transformación
```{r}
ctrl <- trainControl(method="repeatedcv",repeats=5,number = 10)
model_caret_gbm <- train(Claim_Amount~.,data=sample_train, method = "gbm", trControl = ctrl)
model_caret_gbm
```
interaction.depth  n.trees  RMSE      Rsquared  
  1                   50      106.9766  0.02863827
  1                  100      106.9344  0.03001587
  1                  150      106.9473  0.03037122
  2                   50      107.4892  0.03026116
  2                  100      108.2174  0.02772537
  2                  150      108.6716  0.02799684
  3                   50      107.8446  0.03016242
  3                  100      108.8614  0.02762303
  3                  150      109.5989  0.02681473

```{r}
prediction_caret <- predict(model_caret_gbm,test_gbm)
sqrt(mean((test_gbm_output-prediction_caret)^2))
```
RSME= 57.31009

###Con LOG transformación

```{r}
ctrl <- trainControl(method="repeatedcv",repeats=5,number = 10)
model_caret_gbm_log <- train(log(Claim_Amount+1)~.,data=sample_train, method = "gbm", trControl = ctrl)
model_caret_gbm_log
```
interaction.depth  n.trees  RMSE      Rsquared 
  1                   50      1.196408  0.1122657
  1                  100      1.181375  0.1294317
  1                  150      1.174601  0.1367956
  2                   50      1.169310  0.1504169
  2                  100      1.155924  0.1650154
  2                  150      1.152023  0.1683297
  3                   50      1.154251  0.1705702
  3                  100      1.146046  0.1773242
  3                  150      1.143627  0.1794069


```{r}
prediction_caret_log <- predict(model_caret_gbm_log,test_gbm)
sqrt(mean((test_gbm_output-prediction_caret_log)^2))
```
39.35023

##REGRESION SVR CON DATOS EQUILIBRADOS

Vamos a partir del fichero downSample como para el resto de nuestros modelos para ajustar el modelo y comprobar con la predicción sobre el fichero test cual es el RMSE.

###SIN LOG TRANSFORMACIÓN

```{r}
model_gbm3 <- gbm(Claim_Amount~., data=sample_train_down,distribution="gaussian",n.trees = 1000)
summary(model_gbm3)
prediction3 <- predict(model_gbm3,test_gbm,n.trees=1000)
sqrt(mean((test_gbm_output-prediction3)^2))
```
RSME = 122.6814


###CON LOG TRANSFORMACIÓN

```{r}
model_gbm4 <- gbm(log(Claim_Amount+1)~., data=sample_train_down,distribution="gaussian",n.trees = 1000)
summary(model_gbm4)
prediction4 <- predict(model_gbm4,test_gbm,n.trees=1000)
sqrt(mean((test_gbm_output-prediction4)^2))
```
RSME = 39.35612


##K-FOLD CROSS-VALIDATION CON CARET PARA "Least Squares Support Vector Machine" en Datos Equilibrados

###Sin LOG transformación
```{r}
ctrl <- trainControl(method="repeatedcv",repeats=5,number = 10)
model_caret_gbm2 <- train(Claim_Amount~.,data=sample_train_down, method = "gbm", trControl = ctrl)
model_caret_gbm2
min(model_caret_gbm2$results["RMSE"])
```
 interaction.depth  n.trees  RMSE      Rsquared  
  1                   50      281.1104  0.05305882
  1                  100      280.3729  0.05555733
  1                  150      280.2858  0.05559467
  2                   50      281.0628  0.05045275
  2                  100      281.4255  0.04848147
  2                  150      282.2355  0.04516142
  3                   50      281.7959  0.04595110
  3                  100      283.0363  0.04228916
  3                  150      284.7311  0.03805902


```{r}
prediction_caret2 <- predict(model_caret_gbm2,test_gbm)
sqrt(mean((test_gbm_output-prediction_caret2)^2))
```
151.0573

###Con LOG transformación

```{r}
ctrl <- trainControl(method="repeatedcv",repeats=5,number = 10)
model_caret_gbm_log2 <- train(log(Claim_Amount+1)~.,data=sample_train_down, method = "gbm", trControl = ctrl)
model_caret_gbm_log2
min(model_caret_gbm_log2$results["RMSE"])
```
interaction.depth  n.trees  RMSE      Rsquared 
  1                   50      2.026460  0.2636104
  1                  100      1.961306  0.2904541
  1                  150      1.929692  0.3075081
  2                   50      1.947264  0.3041004
  2                  100      1.884237  0.3397761
  2                  150      1.856169  0.3554072
  3                   50      1.900072  0.3342330
  3                  100      1.845919  0.3632660
  3                  150      1.826670  0.3732873


```{r}
prediction_caret_log2 <- predict(model_caret_gbm_log2,test_gbm)
sqrt(mean((test_gbm_output-prediction_caret_log2)^2))
```
39.38117

##Seleccionamos modelo con menor RSME

Prediction4 con log transformación y equilibrado.

```{r}
test$predGBM <- prediction4
write.csv(test,"testdf.csv",row.names = FALSE)
```