---
title: "Analisis Breast Cancer"
author: "Montse Figueiro"
date: "26 de julio de 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Nos da un resumen de como estan medidas las diferentes variables, mostrando:

* Mínimo **min()**
* Primer cuartil
* Mediana
* Tercer cuartil
* Máximo **max()**

La diferencia entre el mínimo y el máximo es el rango. "range()" and "diff()" te permitiría analizar esa diferencia.

Los cuartiles dividen el dataset en 4 partes, cada uno con el mismo número de valores.

La diferencia entre cuartil 1 y cuartil 3 es Rango intercuartil (IQR), se puede calcular con la función **IQR()** 


* **quantile()** nos devuelve los cinco números de summary.
* **quantile(usedcars$price,probs=c(0.01,0.99))** podemos sacar los quantiles que queramos, aquí por ejemplo en el 1% y en el 99%.
* **quantile(usedcars$price,seq(from=0,to=1,by=0.20))** aquí saco el 0%,20%.....

###Interpretación

Esto nos ayuda a ver la dispersión de los datos. El dataset nos dice que el mínimo es 3800 y el máximo 21992, la diferencia entre el mínimo y el Q1 es 7000, la diferencia entre el máximo y el Q3 son 7000 tambien. En cambio la diferencia entre Q1 y Q3 que es el 50% del medio son 2000. Los datos están más estrechamente agrupados en el centro, esto es típico en una distribución normal.

Esto explica porque la media es mucho más grande que la mediana, porque la media es sensible a los valores extremos.

##BOXPLOTS - visualizando los datos

El boxplot muestra los cinco números de summary usando lineas horizontales. La caja muestra Q1, mediana y Q3, leyendo desde abajo hacia arriba. La mediana es la linea en negrita. El mínimo y el máximo está representado por rayas discontinuas. Solo permite extender las rayas discontinuas hasta 1.5 veces el IQR, debajo de Q1 o encima de Q3.

Por ejemplo:
Q1 es 26
Q3 es 56

IQR es 30 (diferencia entre Q3 y Q1)

Q3 + 1.5 * 30 = 101 (no puede llegar hasta 120 que es el máximo)
Q1 - 1.5 * 30 = -9 (llegará hasta el mínimo que es 2)


**boxplot(cars$dist="Dist Cars",ylab="Km")**

```{}
summary(cars$dist)
boxplot(cars$dist,main="Dist Cars",ylab="Km")
```

##HISTOGRAMAS

Divide los valores de las variables en **bins** columnas, un boxplot requiere cada una de las 4 porciones contenga el mismo número de valores. En contraste el histograma usa cualquier número de barras de identico ancho y permite contener dentro diferente número de valores.

```{r}
hist(cars$dist,main="Distance Cars",xlab="Km")

```
Las barras indican la frecuencia de los valores, es este ejemplo vemos que lo más frecuente es que el coche para frenar recorra una distancia de entre 20 y 40 metros.

El histograma tiene una oblicuidad hacia la derecha.

##Distribución NORMAL

###Varianza y Desviación Standar

varianza se define como la media de la suma de las diferencias entre cada valor y la media al cuadrado.

Var= (sum((x-Media)^2))/n
Std= sqrt(Var)
R utiliza n-1 (sample variance), excepto en dataset muy pequeños la diferencia es mínima.

```{r}
var(cars$dist)
sd(cars$dist)
```

Cuando la varianza es muy grande esto indica que los datos se difunden muy ampliamente alrededor de la media. La desviación standar te indica para cada valor cuanto difiere de la media.

###Variables Categóricas

Cuando importamos los datos como astringsAsFactors = FALSE, R deja las variables como character en lugar de convertirlas en factores.
Podemos considerar poner los años como categórica, aunque está como entero cada año es una categoría.

Este tipo de datos se examina usando tablas en lugar de estadísticas.

One-way table : presenta una sola variable categórica.
```{r}
head(iris)
table(iris$Species)
```
Nos hace una lista de las diferentes categorías y cuantos valores hay dentro de cada una.

R puede calcular la tabla de proporciones:
```{r}
species_table <- table(iris$Species)
prop.table(species_table)
species_pct <- prop.table(species_table) *100
round(species_pct,digits=1)
```
Así podemos ver para las diferentes categorías cual es la más frecuente.

###Analizando la Moda
En estadística la moda es el valor que ocurre con más frecuencia. Esta se utiliza frecuentemente en datos categóricos, ya que no se puede usar la media o la mediana.

Una variable puede tener una moda o más
* unimodal
* bimodal
* multimodal

Es mejor relacionar la moda con otras categorias. Hay alguna categoría que domina a otra? Si los colores de coche son plata y negro, considero que estamos hablando de coches de lujo? o coches económicos, cuales se venden con menos opciones de color?
En un histograma la moda sería la barra más alta.

##RELACIÓN ENTRE VARIABLES

###SCATTERPLOT

Es un diagrama que representa la relación entre dos variables mediante circulos,  una variable se representa en el eje "x" y otra variables en el eje "y". 

```{}
plot(x=cars$dist,y=cars$speed,main="Scatterplot of Speed and Distance",ylab="Distance",xlab="Speed")
```
Vemos una relación entre la velocidad y la Distancia recorrida. Tienen una relación positiva, el patrón de circulos es ascendente.
Podría ser una relación negativa si la linea fuera descendiente. Y no tendrían nada que ver si la linea fuera plana.

La **Correlación** mide la relación entre dos variables.

###Tabulación Cruzada

Mide la relación entre dos variables. Como los valores de una variable varian en función de otra variable. El formato es una tabla donde las filas son los niveles de una variable mientras las columna son los niveles de la otra

**CrossTable()**

library(gmodels)
CrossTable(x=train_sample$Cat1,y=train_sample$Cat2)


##Clasificación usando Nearest Neighbors

Mide la similitud de dos muestras midiendo distancias. Se ha utilizado por ejemplo:

* Reconocimiento facial en imagenes y videos
* Predecir cuando una persona va a disfrutar una película que le ha sido recomendada (Netflix)
* Identificar patrones en datos genéticos al detectar una específica proteína.

Por lo general este método de clasificación es bueno cuando la relación entre las variables y las clases objetico es alto, complicado y difícil de entender. Si no hay una clara distinción entre grupos el algorítmo es largo y no es bueno identificando los límites.

###The KNN algorithm

|Fortalezas                  |                               Debilidades  |
|----------------------------|--------------------------------------------|
|Simple y efectivo | No produce un modelo, lo que lo limita para encontrar nuevos conocimientos en las relaciones entre las variables|
|No hace suposiciones sobre la distribucion de los datos subyacente | Fase de clasificación lenta|
| Fase de entrenamiento rápida | Requiere mucha memoria |
| | Variables nominales y missing data requieren procesamiento adicional |

kNN empieza con un training dataset con observaciones classificado en diferentes categorias, y etiquetadas por una variable nominal. Tenemos también un test dataset sin etiquetar que tiene el mismo número de variables que el training data. Para cada observación en el test dataset kNN identifica **k** observaciones en training dataset que son "vecinos", **k** es un entero especificado con anterioridad. A la observación del test datset se le asigna la clase de la mayoria de los k vecinos.

```{r}
ingredient <- c("apple","bacon","banana","carrot","celery","cheese")
sweetness <- c(10,1,10,7,3,1)
crunchiness <- c(9,4,1,10,10,1)
foodtype <- c("fruit","protein","fruit","vegetable","vegetable","protein") 
df <- data.frame(ingredient,sweetness,crunchiness,foodtype)
df
str(df)
```
En este ejemplo se han clasificado las comidas según unas variables, en 3 clases, fruta, carne o verdura.

En este ejemplo solo tenemos 2 variables con lo que podemos representar los datos con un scatterplot:
```{r}
plot(x=df$sweetness,y=df$crunchiness)
```
En este ejemplo hay muy pocas observaciones y no se pueden ver agrupaciones.

###Calculo Distancia

El algoritmo kNN usa la distancia Euclidean. La distancia entre dos observaciones **p** y **q** es la raíz cuadrada de la suma de las distancias al cuadrado.
p1 es el valor de la variable 1 para p
q1 es el valor de la variable 1 para q

$dist(p,q)=\sqrt{\sum_{i=1}^n ((p1-q1)^2+(p2-q2)^2.....))}$

La formula compara los valores para cada variable. Por ejemplo entre el tomate y la banana o entre el tomate y el queso...

Cuando calculemos todas las distancias del tomate al resto de ingredientes, veremos con que ingrediente tiene menos distancia, se llama clasificación 1NN porque k=1, por ejemplo, si el más cercano es la naranja, clasificará al tomate como "fruit". 
Si usamos kNN con k=3, lleva a cabo una votación entre los 3 vecinos más cercanos, los tres más cercanos son naranja, uva y cacahuete, 2 de 3 son frutas, el tomate lo clasificará como "fruit".

###Elección de **k**

Determinar un número correcto de vecinos determinará como de bueno será el modelo para generalizar a datos futuros. 
Escoger una k muy grande reduce el impacto de la varianza causada por el ruido, pero corre el riesgo de ignorar pequeños patrones.

Ejemplo: Tomar una k igual al nº de observaciones, cada observación estaría representada en el voto final. 

Tomar una k=1 implica que permita ruido y outliers, que influencian la clasificación de los ejemplos. Algunas de las observaciones pueden estar mal clasificadas.

El mejor valor para **k** es alguno entre estos dos.

Valores pequeños permiten más complejas decisiones, que mas cuidadosamente encaja en el training data.

Normalmente k está entre 3 y 10. Una práctica común es la raíz cuadrada del número de ejemplos del training dataset. En el ejemplo anterior teníamos 15 ingredientes, 3.87.

Una alternativa es usar diferentes **k** en diferentes test datasets y elegir uno que nos dé la mejor clasificación. si los datos tienen mucho ruido y son muy grande el número de observaciones, la elección de la **k** puede ser menos importante.

###Preparando datos para usar kNN

Las variables se suelen normalizar antes de usar kNN. Esto es así porque el cálculo de las distancias depende de las unidades con las que esté medida la variable. Esto no sería un problema en el caso anterior porque las medidas de las comidas iban de 1 a 10. Pero si por ejemplo una variable estuviera medida en valores de 1 a 1000000, el impacto de las otras variables se vería disminuido.

Necesitamos escalar las variables de manera que cada una de ellas contribuya relativamente igual dentro de la fórmula. En el caso anterior querremos que la última variable esté entre 1 y 10.

* Min-Max normalización

Transforma todas las medidas para que estén entre el rango 0-1.

$Xnew = \frac{X - min(X)}{max(X)-min(X)}$

* Z-score Standarización

Es el valor menos la media dividido por la desviación standar.

$Xnew = \frac{X-\mu}{\sigma}$

Esta fórmula está basada en propiedades de la distribución normal. Reescala cada valor de cada variable según cuanta desviacion standar tenga por encima o por debajo de la media. El resultado se llama z-score.

La distancia Euclidean no está definida para variables nominales, antes hay que pasarlas a numéricas. Una solución típica es utilizar dummy coding, donde 1 indica una categoría y 0 la otra.

Nueva variable llamada "male", tendrá los siguientes valores:

* 1 male
* 0 otherwise

Una variable nominal con *n* categorias puede ser codificada con dummy, por ejemplo si tenemos 3 categorias de tiempo (caliente, templado y frío):

* Caliente = 1 es caliente y 0 el resto
* Templado = 1 es templado y 0 el resto

Si es 0 sabemos que es frío, no necesitamos una tercera variable, tendremos (n-1) variables dummy.
La distancia entre variables dummy es siempre 0 o 1. 

Otra alternativa sería numerar las categorias, 1,2,3 y después normalizar. Quedando 0, 0.5 y 1, esto solo puede ser usado si sabes que la diferencia entre las categorías es proporcional. Si por ejemplo estamos midiendo, pobre, clase media y ricos, la diferencia entre pobre y clase media es más o menos que la diferencia entre clase media y ricos, entonces es mejor utilizar dummy.


###kNN es un algoritmo perezoso?

Un aprendizaje perezoso es un aprendizaje de memoria, no construye un modelo, es una modelo de aprendizaje no paramétrico, no se aprenden parámetros sobre los datos. Limita nuestra capacidad de entender como está clasificando los datos. Aunque puede ser bastante potente.

##Diagnosticando Cancer de pecho con kNN

El exámen de rutina de cáncer de mama permite ser diagnosticado y tratado antes de que se noten los síntomas. El proceso consiste en detectar masas anormales. Si una masa es detectada se extrae una muestra y se analiza (biopsia), para determinar si es maligna o benigna.

Los datos aportados por esas biopsias son los que vamos a analizar para investigar con machine learning.

###Base de Datos

vamos a utilizar el Breast Cancer Wisconsin Diagnostic dataset de la siguiente página:

["BreastCancerDataset"](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)

```{r}
DataCancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",sep=",",header=FALSE)
write.csv(DataCancer,file="DataCancer.csv")
datacancer <- read.csv("DataCancer.csv")
head(datacancer)
datacancer$X <- NULL
str(datacancer)
```
Tenemos 569 observaciones con 32 variables.
```{r}
colnames(datacancer)[1] <- "id"
colnames(datacancer)[c(2,3,4,5,6)] <-c("diagnosis","radius_mean","texture_mean","perimeter_mean","area_mean")
```
La primera variable es un identificador de cada paciente, no tiene que haber registros duplicados.
```{r}
anyDuplicated(datacancer$id)
length(unique(datacancer$id))
```
No nos aporta información para el modelo y hay que excluirla. Suelen ser excluidas siempre las variables ID

```{r}
datacancer <- datacancer[-1]
```
Diagnosis indica si la masa es maligna o benigna
```{r}
table(datacancer$diagnosis)
datacancer$diagnosis <- factor(datacancer$diagnosis,levels=c("B","M"), labels=c("Benign","Malignant"))
head(datacancer)
```
357 casos son benignos y 212 malignos
Queremos saber el porcentaje de casos
```{}
round(prop.table(table(datacancer$diagnosis)) * 100, digits=1)
```
Las otras 30 variables son numéricas
```{r}
summary(datacancer)
colnames(datacancer)[6] <- "smoothness_mean"
```
Nos centramos en 3 variables, radius_mean, area_mean y smoothness_mean, el rango de la primera va desde 6.9 a 28.11, la segunda 143.5 a 2501 y la última de 0.052 a 0.1634, el impacto de la segunda variable va a ser mas que las otras al calcular las distancias.

Vamos a aplicar la normalización.

##Normalización Datos

Vamos a crear la función "normalize":

```{r}
normalize <- function(x){return ((x-min(x))/(max(x)-min(x)))}
```
Prueba (el resultado es el mismo para los dos vectores):
```{r}
normalize(c(1,2,3,4,5))
normalize(c(100,200,300,400,500))
```
Aplicamos la función a las 30 variables, podemos usar **lapply()** para aplicar la función a cada variable y convertir la lista devuelta por lapply en un dataframe con **as.data.frame()**
```{r}
datoscancer_norm <- as.data.frame(lapply(datacancer[2:31],normalize))
summary(datoscancer_norm$radius_mean)
```
##Creando Training y Test Datasets

Cogeremos las primeras 469 filas para el training dataset y las otras 100 para el test dataset. Podemos hacerlo de esta manera si los datos no han sido ordenados y son aleatorios. 
```{r}
dc_train <- datoscancer_norm[1:469,]
dc_test <- datoscancer_norm[470:569,]
```
Cuando construimos los dataset excluimos la columna diagnosis. Esta es la columna diagnosis para train y test:

```{r}
datos_train_labels <- datacancer[1:469,1]
datos_test_labels <- datacancer[470:569,1]
```
Utilizaremos esta columna más adelante para evaluar nuestra clasificación.

##Training un modelo en nuestros datos

La función knn() está en el paquete "class", para cada observación la función identificará el k-vecino más cercano, usando la distancia Euclidean, donde "k" es un número especificado.
```{r}
library(class)
```
Sintaxis de la función knn():

p <-  knn(train,test,class,k)
* train es una data frame con los datos numericos (training dataset)
* test es un data frame con los datos numéricos (test dataset)
* class es un vector de factores con la clasificación de cada fila en el train dataset
* k es un entero que indica el numero de vecinos.

```{r}
datacancer_test_pred <-  knn(train=dc_train,test=dc_test,cl=datos_train_labels,k=3)
```
Nos devuelve un vector con la prediccion para cada fila del test dataset (factores benignos o malignos)
##Evaluación del modelo

El siguiente paso es evaluar como de bien ha ajustado las predicciones en el vector "datacancer_test_pred"

```{r}
library(gmodels)
```
Podemos cruzar el **datos_test_labels** con el **datacancer_test_pred** para ver si coinciden los dos vectores.
```{r}
CrossTable(x=datos_test_labels,y=datacancer_test_pred, prop.chisq = FALSE )
```
El la celda superior izquierda están los TN, 72 valores de 100, indican los casos que eran Benignos y el knn algoritmo los identifica correctamente. 
La celda inferior derecha son los casos Malignos que el algoritmo clasifica correctamente.TP.

Los otros 6 casos son los que no ha clasificado correctamente.Hay un caso el el nivel inferior izquierdo que son los Falsos Negativos, el algoritmo los clasificó como Benignos y en realidad fueron Positivos. Los otros 5 casos son Falsos Positivos, los clasificó como Malignos pero realmente son Benignos. 

Un 1% son falsos Negativos, son los verdaderamente peligrosos. Podemos probar otra iteracion del modelo para ver si podemos reducir el número de valores incorrectos.

##Improving Model performance

vamos a intentar dos variaciones:
* Un método alternativo para escalar las variables numéricas
* Probaremos diferentes valores de k

###Z-score estandarización

Con tumores malignos podemos ver algunos "outliers" valores extremos, estos tienen que tener más peso en el cálculo de las distancias.
Las función scale() utiliza la estandarización z-score.
```{r}
data_z <- as.data.frame(scale(datacancer[-1]))

summary(data_z$area_mean)
```
La media de una variable estandarizada z-score tiene que ser 0. Una variable mayor que 3 y menor que -3 indica valores extremos raros.

Como hicimos antes hay que dividir los datos en 2 partes:

```{r}
train <- data_z[1:469,]
test <- data_z[470:569,]
train_labels <- datacancer[1:469,1]
test_labels <- datacancer[470:569,1]
test_pred <- knn(train=train,test=test,cl=train_labels,k=21)
CrossTable(x=test_labels,y=test_pred,prop.chisq = FALSE)
```
En este caso hemos clasificado el 98% de los casos correctamente, con respecto al 94% anterior. Pero donde antes teníamos 1 caso de Falso Negativo ahora tenemos 2.

##Alternativos valores de K

* k=1
```{r}
test_pred_1 <- knn(train=train,test=test,cl=train_labels,k=1)
CrossTable(x=test_labels,y=test_pred_1,prop.chisq = FALSE)
```
* k=5
```{r}
test_pred_5 <- knn(train=train,test=test,cl=train_labels,k=5)
CrossTable(x=test_labels,y=test_pred_5,prop.chisq = FALSE)
```
* k=11
```{r}
test_pred_11 <- knn(train=train,test=test,cl=train_labels,k=11)
CrossTable(x=test_labels,y=test_pred_11,prop.chisq = FALSE)
```
* k=15
```{r}
test_pred_15 <- knn(train=train,test=test,cl=train_labels,k=15)
CrossTable(x=test_labels,y=test_pred_15,prop.chisq = FALSE)
```
* k=25
```{r}
test_pred_15 <- knn(train=train,test=test,cl=train_labels,k=15)
CrossTable(x=test_labels,y=test_pred_15,prop.chisq = FALSE)
```
|K-value| Falsos Negativos | Falsos Positivos | Porcentaje Clasif.Incorrecta |
|-------|------------------|------------------|------------------------------|
|1|2|4|6%|
|5|0|4|4%|
|11|1|1|2%|
|15|2|0|2%|
|25|2|0|2%|

##Summary

Resumiendo KNN utiliza el cálculo de distancias para coger una nueva observación y colocarla en vecino más similar. Es sencillo y permite como en el ejemplo anterior clasificar correctamente el 98% de los datos.

#Aprendizaje probabilistico - Clasificación Naive Bayes

